\documentclass[a4paper, 10pt]{article}
\usepackage[a4paper,left=2.5cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage[utf8]{inputenc} % Change according your file encoding
\usepackage{graphicx}
%\usepackage[demo]{graphicx}
\usepackage{url}

\usepackage{float}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{todonotes}

\usepackage{listings}

\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},  
    breakatwhitespace=false,         
    basicstyle=\scriptsize,
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    frame=single,
    morekeywords = {MPI_Comm_size},
}



\lstset{style=mystyle}

%opening
\title{\textbf{Parallelism: Assignment 2\\Solving the Heat Equation \\using several Parallel Programming Models}}
\author{Ignacio Encinas Rubio, Adrián Jiménez González\\\{ignacio.encinas,adrian.jimenez.g\}.estudiantat.upc.edu}
\date{\normalsize\today{}}

\begin{document}

\maketitle

%\begin{center}
  %Upload your report in PDF format.
  
  %Use this LaTeX template to format the report.
  
	%A compressed file (.tar.gz) containing all your source code files must be submitted together with this report.
%\end{center}

\section{Introduction}

\section{Parallelization}

In this section we will explain the steps we followed to achieve the parallelization of both solvers, Jacobi and Gauss-Seidel. In the following sections, we are going to show how the parallel codes have been done in each Parallel Programming Model for each solver.

\subsection{OpenMP}

\subsubsection{Jacobi}

  Parallelizing the Jacobi solver with OpenMP just requires to modifying the \textit{solver-omp.c} file. The \textit{heat-omp.c} file does not need any modification. 
Jacobi does not have any data dependencies inside an iteration, it just requires that the iterations themselves are carried out in order, so the parallelization can be achieved by simply adding a \texttt{\#pragma omp} as shown below at Listing \ref{lst:jacobiopenmp}.

\begin{lstlisting}[language=c, caption={OpenMP pragma for Jacobi parallelization}, label={lst:jacobiopenmp}]
    #pragma omp parallel for collapse(2) private(diff) reduction(+:sum)
    for (int ii=0; ii<nbx; ii++)
        for (int jj=0; jj<nby; jj++) 
            for (int i=1+ii*bx; i<=min((ii+1)*bx, sizex-2); i++) 
                for (int j=1+jj*by; j<=min((jj+1)*by, sizey-2); j++) {
                         utmp[i*sizey+j]= 0.25 * (u[ i*sizey     + (j-1) ]+  // left
                                                  u[ i*sizey     + (j+1) ]+  // right
                                                  u[ (i-1)*sizey + j     ]+  // top
                                                  u[ (i+1)*sizey + j     ]); // bottom
                         diff = utmp[i*sizey+j] - u[i*sizey + j];
                         sum += diff * diff; 
                 }
\end{lstlisting}

Jacobi does not require performing the computation in ``blocks'' because it doesn't have internal dependencies like Gauss-Seidel. 
So we will fuse both for loops with the \texttt{collapse(2)} clause. The next clause we use is \texttt{private(diff)} in which we specify 
that each thread will have its own copy of the diff variable. Last, we have the \texttt{reduction(+:sum)} specifying 
the way we want to reduce the values obtained by each thread. In this case, we want to sum (+) the values for each \texttt{sum} when we 
finish the parallel section in order to return the residual of the solver.

\clearpage

\subsubsection{Gauss-Seidel}

The parallelization of Gauss-Seidel solver using OpenMP is quite different. In this case, we are need tasks in order to parallelize while 
respecting the dependencies of that solver. We introduce a new proxy variable \texttt{block}\footnote{Shown as \texttt{b} in the listing}, used to indicate the dependencies between tasks and mark whenever they're fulfilled. See Listing \ref{lst:seidelopenmp} for more details.


{\huge{REVISAR}}



\begin{lstlisting}[language=c, caption={OpenMP pragma for Gauss-Seidel parallelization}, label={lst:seidelopenmp}]
    int b[nbx][nby];

#pragma omp parallel
#pragma omp single
{
  for (int ii=0; ii<nbx; ii++) {
    for (int jj=0; jj<nby; jj++) {
  #pragma omp task depend(in: b[ii-1][jj], b[ii][jj-1]) depend(out: b[ii][jj]) private(diff, unew) 
      {
        double omp_sum = 0.0;
        for (int i=1+ii*bx; i<=min((ii+1)*bx, sizex-2); i++) {
          for (int j=1+jj*by; j<=min((jj+1)*by, sizey-2); j++) {
              unew= 0.25 * (u[i*sizey	    + (j-1)]+  // left
                            u[i*sizey	    + (j+1)]+  // right
                            u[(i-1)*sizey + j    ]+  // top
                            u[(i+1)*sizey + j    ]); // bottom
              diff = unew - u[i*sizey+ j];
              omp_sum += diff * diff; 
              u[i*sizey+j]=unew;
          } 
        }
        #pragma omp atomic
        sum += omp_sum;
      }
    }
  }
}
\end{lstlisting}

  In this case, not only the \texttt{\#pragmas} have been added. Some code needs to be changed in order to obtain the same behaviour as sequential code. First, as we have mentioned, we have introduced a new variable \texttt{block} to manage the dependencies on the \texttt{\#pragma omp task}. Each task will depend on the ``top" and ``left" blocks, indicated in the \texttt{depend(in:var\_list)} clause. The task produces \texttt{block[ii][jj]} with the \texttt{depend(out:var\_list)} clause. Then, when the tasks of the left and the top from a block are finished, this task will be able to start, allowing us to exploit wavefront parallelism.

  Also, we need to indicate that region as parallel with \texttt{\#pragma omp parallel} to create the threads to execute that region of code. Inside that region we have a race condition in the \texttt{sum} variable. In order to solve it, we create a new variable where we make the operations for each thread, and then with a \texttt{\#pragma omp atomic} we avoid that critical section, and we obtain the residual value correctly.

\clearpage

\subsection{MPI}

\subsubsection{Jacobi}

For the parallelization of Jacobi using MPI, we need to manage all the boundaries or halos from each of the nodes. First, we need to split the data between all the nodes we use. For this, we need to send the part of \texttt{param.u} and \texttt{param.uhelp} to the corresponding children node\footnote{Some not modified code has been deleted from the listings in order to have the report more clear}. Assuming $rows \equiv 0 $ (mod numprocs): $rowsWorkers = rows / numprocs + 2$. The +2 corresponds to the first and last rows, that are part of the halo.

\begin{lstlisting}[language=c, caption={Sending/Receiving initial data to all nodes}]
/* ROOT NODE (rank == 0) */
for (int i=0; i<numprocs; i++) {
  MPI_Send(&param.u[np*rowsWorkers*i], np*(rowsWorkers+2), MPI_DOUBLE, i, 0, MPI_COMM_WORLD);
  MPI_Send(&param.uhelp[np*rowsWorkers*i], np*(rowsWorkers+2), MPI_DOUBLE, i, 0, MPI_COMM_WORLD);
}    

/* (rank != 0) */
MPI_Recv(&u[0], (rows+2)*(columns+2), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &status);
MPI_Recv(&uhelp[0], (rows+2)*(columns+2), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &status);
\end{lstlisting}

The root node sends inside a for loop to all the nodes. The children nodes only need to receive once as it is shown above.


Inside the iteration part, we need to send and receiving the halo from the contiguous nodes. This is different for 3 types of nodes:

\begin{itemize}
  \item Node 0: only sends to and receives from the next node.
  \item Intermediate nodes: send to and receive from previous and next nodes.
  \item Last node: only sends and receives from previous node.
\end{itemize}

\begin{lstlisting}[language=c, caption={Communications between nodes inside the iterations}]
/* ROOT NODE (rank == 0) */

MPI_Send(&param.u[np*rowsWorkers], np, MPI_DOUBLE, myid+1, 0, MPI_COMM_WORLD);
MPI_Recv(&param.u[np*(rowsWorkers+1)], np, MPI_DOUBLE, myid+1, 0,  MPI_COMM_WORLD, &status);

residual = relax_jacobi(param.u, param.uhelp, rowsWorkers+2, np); 

/* (rank != 0) */

MPI_Send(&u[columns+2], columns + 2, MPI_DOUBLE, myid-1, 0, MPI_COMM_WORLD);

MPI_Recv(&u[0], columns + 2, MPI_DOUBLE, myid-1, 0, MPI_COMM_WORLD, &status);
                    
if(myid != numprocs - 1){
  MPI_Send(&u[rows*(columns+2)], columns+2, MPI_DOUBLE, myid+1, 0, MPI_COMM_WORLD);
  MPI_Recv(&u[(rows+1)*(columns + 2)], columns+2, MPI_DOUBLE, myid+1, 0, MPI_COMM_WORLD, &status);
}

residual = relax_jacobi(u, uhelp, rows+2, np);
\end{lstlisting}

After obtaining the residual of the heat equation, we need to sum the value from all the nodes to use it as break condition of the loop, so all the nodes could finish at the same time. This part is identical in all the nodes.

\begin{lstlisting}[language=c, caption={Allreduce for residual value}]
double res;
MPI_Allreduce(&residual, &res, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);
residual = res;
\end{lstlisting}

Once we have finished the iterative part, we need to send each part of the image computed in the children to the parent.

\begin{lstlisting}[language=c, caption={Communication of the computed image to the parent}]
/* ROOT NODE (rank == 0) */

for(int i = 1; i < numprocs; i++){
  MPI_Recv(&param.u[np*(rowsWorkers*i + 1)], np*(rowsWorkers), MPI_DOUBLE, i, i, MPI_COMM_WORLD, &status);
}
/* (rank != 0) */

MPI_Send(&u[np], np*rows, MPI_DOUBLE, 0, myid, MPI_COMM_WORLD);}

residual = relax_jacobi(u, uhelp, rows+2, np);
\end{lstlisting}


\subsubsection{Gauss-Seidel}

For the parallelization of this solver, most of the work have been already done for Jacobi method. Parts like splitting the data between nodes, reduction of the residual for breaking condition and merging the computed image of children nodes in the parent node are common to Gauss-Seidel. Only the communications between nodes in the iterative part are different, as it is shown in Listing \ref{lst:gauss-mpi}.

\begin{lstlisting}[language=c, caption={Communications between nodes Gauss-Seidel}, label={lst:gauss-mpi}]
/* ROOT NODE (rank == 0) */
residual = relax_gauss(param.u, rowsWorkers+2, np, myid, numprocs);
 
MPI_Recv(&param.u[(rowsWorkers + 1)*(np)], np, MPI_DOUBLE, myid+1, 0, MPI_COMM_WORLD, &status);

/* (rank != 0) */

residual = relax_gauss(u, rows + 2, np, myid, numprocs);   

MPI_Send(&u[np], np, MPI_DOUBLE, myid-1, 0, MPI_COMM_WORLD);
if(myid != numprocs - 1){
  MPI_Recv(&u[(rows+1)*np], np, MPI_DOUBLE, myid+1, 0, MPI_COMM_WORLD, &status);
}

residual = relax_jacobi(u, uhelp, rows+2, np);
\end{lstlisting}

For Gauss-Seidel we have also different types of nodes in order to send or receive between iterations. This communications only appears when the Gauss solver has finished with all the blocks of its node:

\begin{itemize}
  \item Node 0: only receives the lower boundary from next node.
  \item Intermediate nodes: receives from next node the lower boundary and sends to the previous node the first computed row.
  \item Last node: only sends to the previous node the first computed row.
\end{itemize}


The communications between nodes after a block is computed are done inside the Gauss-Seidel's kernel. We use the outer for loops of the kernel that compute the solver in blocks to send and receive the necessary information for each block. To be able to differentiate the behaviour of the different nodes that we have, \textit{relax\_gauss} definition has been modified to bring the solver the context of which node is executing and it make the different communications. We also need to initialize \texttt{\&status} variable for the communications. See Listing \ref{lst:gauss-mpi-kernel}.

\begin{lstlisting}[language=c, caption={Communications between nodes Gauss-Seidel inside kernel}, label={lst:gauss-mpi-kernel}]
double relax_gauss(double *u, unsigned sizex, unsigned sizey, int rank, int numprocs) //rank and numprocs passed as parameters
{
MPI_Status status;
for (int ii=0; ii<nbx; ii++)
   for (int jj=0; jj<nby; jj++) {
     if(ii == 0 && rank != 0){
       MPI_Recv(&u[jj * by], by, MPI_DOUBLE, rank-1, jj, MPI_COMM_WORLD, &status);
     }
     for (int i=1+ii*bx; i<=min((ii+1)*bx, sizex-2); i++) 
       for (int j=1+jj*by; j<=min((jj+1)*by, sizey-2); j++) {
        /* COMPUTE EQUATION */
       }
     if(ii == nbx - 1 && rank != numprocs - 1) {
       MPI_Send(&u[((sizex - 2) * sizey) + jj*by], by, MPI_DOUBLE, rank+1, jj, MPI_COMM_WORLD);
    }
  } 
}
\end{lstlisting}

As we can see, we use ii to differentiate if the block is in the top side so the block must receive the upper boundary from previous node in case its rank is different to 0. Once we receive this data, we can compute the equation.

Blocks of the same node do not need to send or receive data between them as they are in shared memory. The blocks of the bottom side (\texttt{ii == nbx-1}) must send their last computed row to the next node in order to the upper block of the next node could start its execution. In this way we obtain the wavefront behaviour.



\subsection{CUDA}

\section{Parallel solution}

\subsection{OpenMP}

\subsection{MPI}

\subsection{CUDA}


\end{document}
