\documentclass[a4paper, 10pt]{article}
\usepackage[a4paper,left=2.5cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage[utf8]{inputenc} % Change according your file encoding
\usepackage{graphicx}
%\usepackage[demo]{graphicx}
\usepackage{url}

\usepackage{float}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{todonotes}

\usepackage{listings}

\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},  
    breakatwhitespace=false,         
    basicstyle=\scriptsize,
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    frame=single,
    morekeywords = {MPI_Comm_size},
}



\lstset{style=mystyle}

%opening
\title{\textbf{Parallelism: Assignment 1\\A Distributed Data Structure}}
\author{Ignacio Encinas Rubio, Adrián Jiménez González\\\{ignacio.encinas,adrian.jimenez.g\}.estudiantat.upc.edu}
\date{\normalsize\today{}}

\begin{document}

\maketitle

%\begin{center}
  %Upload your report in PDF format.
  
  %Use this LaTeX template to format the report.
  
	%A compressed file (.tar.gz) containing all your source code files must be submitted together with this report.
%\end{center}

\section{Introduction}

This is the report corresponding to the first assignment of the parallelism module, performed by Ignacio Encinas Rubio and Adrián Jiménez González. The source code submitted through Atenea has additional comments that have been omitted in our code listings for clarity's sake.


\section{A distributed data structure}

   In this section we will briefly comment the code added to the template version in order to
   make the algorithm work. We will show the minimum number of lines of code possible to follow the reasoning.

\subsection{Parallel data structure}


    The first step just after calling \textit{MPI\_Init} is to get information from the MPI runtime in order to know how many processes are running the same program and our ID inside that group. 

    \begin{minipage}{.45\textwidth}
	\begin{lstlisting}[language=c, caption={Template S1, S2}]
MPI_Comm_rank(,); /* Statement S1 */
MPI_Comm_size(,); /* Statement S2 */
	\end{lstlisting}
    \end{minipage}\hfill
    \begin{minipage}{.45\textwidth}
	\begin{lstlisting}[language=c, caption={Correct S1, S2}]
MPI_Comm_rank(MPI_COMM_WORLD, &rank); 
MPI_Comm_size(MPI_COMM_WORLD, &size); 
	\end{lstlisting}
    \end{minipage}

    Next, we have to figure out how to properly fill these so called ``ghost points''. 


    \begin{lstlisting}[language=c, caption={Template S3, S4, S5}]
MPI_Recv(xlocal[0],maxn, MPI_DOUBLE, rank - 1, 0, , &status); /* S3 */
MPI_Send(xlocal[1], , , rank - 1, 1, ); /* S4 */
MPI_Recv(xlocal[maxn/size+1], , , rank + 1, 1, , &status); /* S5 */
    \end{lstlisting}

    \begin{lstlisting}[language=c, caption={Correct S3, S4, S5}]
MPI_Recv(xlocal[0], maxn, MPI_DOUBLE, rank - 1, 0, MPI_COMM_WORLD, &status); /* S3 */
MPI_Send(xlocal[1], maxn, MPI_DOUBLE, rank - 1, 1, MPI_COMM_WORLD); /* S4 */
MPI_Recv(xlocal[maxn/size+1],maxn,MPI_DOUBLE,rank+1,1,MPI_COMM_WORLD,&status); /* S5 */
    \end{lstlisting}
    \begin{itemize}
	\item \textbf{S3}: missing the MPI Communicator to be used.
	\item \textbf{S4}: missing the nº of elements to be sent (elements per row), the data type and 
	    the communicator.
	\item \textbf{S5}: missing the nº of elements, datatype and communicator.
    \end{itemize}

    Last, we have to figure how to sum the amount of errors detected by each MPI Process. For that, we use \textit{MPI\_Reduce}. Its missing the number of elements to be sent by each process, the datatype, the operation to be performed on the information, the root process and the communicator.

    \begin{lstlisting}[language=c, caption={Template S6}]
MPI_Reduce( &errcnt, &toterr,  ,  ,  ,  ,   ); /* S6 */
    \end{lstlisting}

    \begin{lstlisting}[language=c, caption={Correct S6}]
MPI_Reduce(&errcnt, &toterr, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD); /* S6 */
    \end{lstlisting}



\subsection{Nonblocking parallel data structure}

    \begin{lstlisting}[language=c, caption={Template S7, S8, S9, S10}]
MPI_Isend( , , MPI_DOUBLE, rank + 1, 0, MPI_COMM_WORLD, &r[nreq++] ); /* S7 */
MPI_Irecv( , , MPI_DOUBLE, rank - 1, 0, MPI_COMM_WORLD, &r[nreq++] ); /* S8 */
MPI_Isend( , , MPI_DOUBLE, rank - 1, 1, MPI_COMM_WORLD, &r[nreq++] ); /* S9 */
MPI_Irecv( , , MPI_DOUBLE, rank + 1, 1, MPI_COMM_WORLD,); /* S10 */
    \end{lstlisting}

    \begin{lstlisting}[language=c, caption={Correct S7, S8, S9, S10}]
MPI_Isend(xlocal[maxn / size], maxn, MPI_DOUBLE, rank + 1, 0, MPI_COMM_WORLD, &r[nreq++]); /* S7 */
MPI_Irecv(xlocal, maxn, MPI_DOUBLE, rank - 1, 0, MPI_COMM_WORLD, &r[nreq++]); /* S8 */
MPI_Isend(xlocal[1], maxn, MPI_DOUBLE, rank - 1, 1, MPI_COMM_WORLD, &r[nreq++]); /* S9 */
MPI_Irecv(xlocal[maxn / size + 1], maxn, MPI_DOUBLE, rank + 1, 1, MPI_COMM_WORLD, &r[nreq++]); /* S10 */
    \end{lstlisting}

    \begin{lstlisting}[language=c, caption={Template S11}]
MPI_Reduce( , , 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD); /* S11 */
    \end{lstlisting}

    \begin{lstlisting}[language=c, caption={Correct S11}]
MPI_Reduce(&errcnt, &toterr, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD); /* S11 */
    \end{lstlisting}

\subsection{SendReceive parallel data structure}

    For the SendReceive implementation the missing information was: the size of the message, the datatype and the communicator to be used.

    \begin{lstlisting}[language=c, caption={Template S12}]
MPI_Sendrecv(xlocal[1], , , prev_nbr, 1, xlocal[maxn/size+1], , , next_nbr, 1, , status);   /* S12 */
    \end{lstlisting}

    \begin{lstlisting}[language=c, caption={Correct S12}]
MPI_Sendrecv(xlocal[1], maxn, MPI_DOUBLE, prev_nbr, 1, xlocal[maxn / size + 1], maxn, MPI_DOUBLE, next_nbr, 1, MPI_COMM_WORLD, &status); /* S12 */
    \end{lstlisting}

\section{A simple Jacobi iterative method}

\textbf{Q3}: First of all, the space for the halo has to be allocated, and that's why we need two extra rows in our xlocal submatrix. Then, we have to avoid computing on the halo by restricting our nested for loops from computing new values for the outer columns. In order to avoid computing new values for the outer rows, we restrict the range tweaking the values i\_first, i\_{last} for our first and last MPI processes, making them ignore their first and last row respectively, because those are part of the halo. 

\subsection{Jacobi}
    In order to make our code clearer, this part of the assignment introduced MPI\_PROC\_NULL. This is useful because it lets us avoid filling our code with ifs considering corner cases. In this case, last process doesn't have a ``next neighbour'' so he can't send him anything. 
    \begin{lstlisting}[language=c, caption={Template S13}]
next_nbr = ; /* S13 */
    \end{lstlisting}

    \begin{lstlisting}[language=c, caption={Correct S13}]
next_nbr = MPI_PROC_NULL; /* S13 */
    \end{lstlisting}


Next step is to compute the total difference norm from the current matrix to the previous one in order to know when to stop. 

\textbf{Q1}: As every process has to know this, this reduction has to be shared with all of them. Thats why we need ``Allreduce''. 

\textbf{Q2}: The other alternative would be to use MPI\_Reduce and then send the result to every process with a broadcast (for example).
    \begin{lstlisting}[language=c, caption={Template S14}]
MPI_Allreduce( &diffnorm, .....); /* Statement S14 */
    \end{lstlisting}

    \begin{lstlisting}[language=c, caption={Correct S14}]
MPI_Allreduce(&diffnorm, &gdiffnorm, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD); /* S14 */
    \end{lstlisting}


To complete our parallel Jacobi implementation the last step is to gather the disjoint computations performed by every working process. For that we use an MPI\_Gather, where the missing information was: the number of elements to send, the datatype and the root process.
    \begin{lstlisting}[language=c, caption={Template }]
MPI_Gather( xlocal[1], , , x, maxn * (maxn/size), , , MPI_COMM_WORLD ); /* S15 */
    \end{lstlisting}

    \begin{lstlisting}[language=c, caption={Correct }]
MPI_Gather(xlocal[1], maxn * (maxn / size), MPI_DOUBLE, x, maxn * (maxn / size), MPI_DOUBLE, 0, MPI_COMM_WORLD); /* S15 */
    \end{lstlisting}

\subsection{Nonblocking Jacobi}

The error in statement 16 is using providing status instead of its address.
    \begin{lstlisting}[language=c, caption={Template S16}]
MPI_Wait( &r[2], status ); /* S16 (Fix the error) */
    \end{lstlisting}

    \begin{lstlisting}[language=c, caption={Correct S16}]
MPI_Wait(&r[2], &status); /* S16 */
    \end{lstlisting}

For the MPI\_Waitall we have to provide the array of requests and array of statuses, and for the MPI\_Wait we need to specify which request we want to wait for. As we want to wait for xlocal[1] to be sent, we go to the MPI call that sends xlocal[1] and wait for the MPI\_Request associated with it. 
    \begin{lstlisting}[language=c, caption={Template S17, S18}]
MPI_Waitall(nreq, , ); /* S17 */
MPI_Wait(&r[ ], &status); /* S18 */
    \end{lstlisting}

    \begin{lstlisting}[language=c, caption={Correct S17, S18}]
MPI_Waitall(nreq, r, statuses); /* S17 */
MPI_Wait(&r[3], &status); /* S18 */
    \end{lstlisting}

The MPI\_Iallreduce is missing the local variable for each process that holds the difference norm to be shared with the other processes and the element of r to be used to hold the operation's information.
    \begin{lstlisting}[language=c, caption={Template S19}]
MPI_Iallreduce( , &gdiffnorm, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD, &r[ ]); /* S19 */
    \end{lstlisting}

    \begin{lstlisting}[language=c, caption={Correct S19}]
MPI_Iallreduce(&diffnorm, &gdiffnorm, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD, &r[0]); /* S19 */
    \end{lstlisting}

MPI\_Igather is missing the sending buffer and the request handle. As we need to retreive every processed element by every process, the starting point is xlocal[1], just after the ghost points comming from other processes, and we'll have to send maxn elements per row times the number of processed rows.
    \begin{lstlisting}[language=c, caption={Template 20}]
MPI_Igather( , maxn * (maxn/size),MPI_DOUBLE, x, maxn * (maxn/size), MPI_DOUBLE, 0, MPI_COMM_WORLD, &r[ ]); /* S20 */
    \end{lstlisting}

    \begin{lstlisting}[language=c, caption={Correct 20}]
MPI_Igather(xlocal[1], maxn * (maxn / size), MPI_DOUBLE, x, maxn * (maxn / size), MPI_DOUBLE, 0, MPI_COMM_WORLD, &r[0]); /* S20 */
    \end{lstlisting}


\subsection{Jacobi vr}

\textbf{Q4}: $maxn = kP + t \ | \ 0 \leq t < P$ is distributed by assigning $k$ rows for each MPI process. This part is perfectly balanced\footnote{Assuming computational load is independent from the actual data}, but the remainder $t$ can't be equally distributed, so we have to think about a smart way to distribute it. The proposed schema is to give one extra row for each MPI Process\footnote{Starting from the top rank} until we've assigned $t$ extra rows, achieving $maxn$ rows assigned.

The load balancing strategy is determined in the Assignment, so we just have to fill that for S21. In S22 we have to provide the missing parameters, that are: the rank ID of the MPI Process, and the total amount of MPI Processes.
    \begin{lstlisting}[language=c, caption={Template S21, S22}]
return (rowsTotal / mpiSize) + ...... ; /* Statement S21 */
nrows  = getRowCount(maxn, , ); /* Statement S22 */
    \end{lstlisting}

    \begin{lstlisting}[language=c, caption={Correct S21, S22}]
return (rowsTotal / mpiSize) + (rowsTotal % mpiSize > mpiRank); /* Statement S21 */
nrows = getRowCount(maxn, rank, size); /* Statement S22 */
    \end{lstlisting}

In this simple case the first Gather is a bit unnecessary as we could compute everything from the root node, but it might not be the case in some other programs. MPI\_Gather gets from every working process the amount of rows they're gonna process, and stores it into an array. Then, we will use that array in order to do the gather with a variable ammount of elements per process.

The missing information in the MPI\_Gather is: the number of elements to be sent and datatype.
The missing information in the MPI\_Gatherv is: the number of elements to be sent and the array with the number of elements to be received from every process.
    \begin{lstlisting}[language=c, caption={Template S23, S24}]
MPI_Gather( &lcnt,  ,   , recvcnts, 1, MPI_INT, 0 , MPI_COMM_WORLD ); /* Statement S23 */
MPI_Gatherv( xlocal[1],   , MPI_DOUBLE, x,    , displs, MPI_DOUBLE, 0, MPI_COMM_WORLD ); /* Statement S24 */
    \end{lstlisting}

    \begin{lstlisting}[language=c, caption={Correct S23, S24}]
MPI_Gather(&lcnt, 1, MPI_INT, recvcnts, 1, MPI_INT, 0, MPI_COMM_WORLD); /* Statement S23 */
MPI_Gatherv(xlocal[1], lcnt, MPI_DOUBLE, x, recvcnts, displs, MPI_DOUBLE, 0, MPI_COMM_WORLD); /* S24 */
    \end{lstlisting}

\end{document}
